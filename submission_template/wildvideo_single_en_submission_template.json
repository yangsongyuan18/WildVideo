{
  "task": "wildvideo_single_en",
  "split": "test",
  "lang": "en",

  "model_info": {
    "model_name": "YOUR_MODEL_NAME",
    "model_size": "7B",
    "vision_encoder": "YOUR_VISION_ENCODER",
    "notes": "e.g., max_frames_num=64, conv_template=qwen_1_5, mm_spatial_pool_mode=average"
  },

  "judge_info": {
    "judge_type": "LLM-as-a-judge",
    "judge_model_name": "API_NAME",
    "sys_prompt_version": "v1.0",
    "api_provider": "openai-or-proxy-name"
  },

  "metric": {
    "name": "wildvideo_single_en_acc",
    "value": 0.44763920309421296,
    "description": "Accuracy judged by LLM on WildVideo single-turn English test split (0~1)."
  },

  "extra_stats": {
    "total_judged": 13703,
    "correct_judged": 6134,
    "failed_judged": 31,
    "acc_raw": 0.44763920309421296
  },

  "run_meta": {
    "date": "YYYY-MM-DD",
    "author": "your_name_here",
    "codebase": "lmms_eval",
    "wildvideo_version": "v1.0"
  }
}
